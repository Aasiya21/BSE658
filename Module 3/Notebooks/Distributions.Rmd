---
title: "Inferential Statistics: Probability & Distributions"
output: html_notebook
---

So far we have discussed about descriptive statistics - summarizing data and plotting it. But in order gain the power of making inferences, we will be strating with inferential statistics.

#### Pre-requisite: Probability

##### Difference between probability and statistics**
Probability theory is a branch of mathematics that tells you how often different kinds of events will happen. For eg. What are the chances of a fair coin coming up heads 10 times in a row? or What are the chances that I’ll win the lottery?

In each case the “truth of the world” is known. We know that the coin is fair, so there’s a 50% chance that any individual coin flip will come up heads. We know that the lottery follows specific rules. The critical point is that probabilistic questions start with a known model of the world, and we use that model to do some calculations. *[Chapter 9, Navarro D.]*

- - - -
**A short note on Models**

A model is a simplified representation of a system. For example, the map of a city represents a city in a simplified fashion. A map providing as much detail as the original city would not only be impossible to construct, it would also be pointless. Humans build models, such as maps and statistical models, to make their lives simpler. *[Chapter 3, Winter B.]*
- - - -

But even though we know the models like `P(heads) = 0.5`, we do not know the data (Whetehr heads will come 10 times or 3 times). However, for statistics, it is the opposite. We have the data and we want to infer the truth about the world. For eg., If my friend flips a coin 10 times and gets 10 heads, are they playing a trick on me? or If the lottery commissioner’s spouse wins the lottery, how likely is it that the lottery was rigged?

We want to figure out which is the true model of the world. Is it *P(heads) = 0.5* or is it *P(heads) $\ne$ 0.5*?

##### What is probability really?

**The frequentist view**

![Frequentist_graph](Fig4.png)

According to the frequentist view, flip a fair coin over and over again, and as N grows large (approaches infinity, denoted N Ñ 8), the proportion of heads will converge to 50%.

 *Advantages*
 -  It is objective: the probability of an event is necessarily grounded in the world.
 -  It is unambiguous: any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.

But it all depends on infinite flips of coin. Do infinities really exist in the physical universe? What about the probability for a single non-repeatable event like the chances of rain on 21 September 2021?

**The Bayesian view**

Bayesian view is subjectivist view. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. But how to operationalize this 'degree of belief'? 

One way is to use 'rational gambling'. So a “subjective probability” will be operationalized in terms of what bets you're willing to accept.

 *Advantage*
 - You don’t need to be limited to those events that are repeatable.
 
 *Disadvantage*
 - Can’t be purely objective – specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician, but there has to be an **intelligent agent** out there that believes in things. 


In short, frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to) while the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).

##### Definitions

Refer to the example described in *Section 9.3.1, Navarro D.* for the following content.

**Elementary event:** Every time we make an observation (e.g., every time I put on a pair of pants), then the outcome will be one and only one of these events.

**Sample space:** The set of all possible events (e.g., the wardrobe)

**Probability:** Numbers between 0 and 1.

For an event X, the probability of that event P(X) is a number that lies between 0 and 1. The bigger the value of P(X), the more likely the event is to occur.

If P(X) = 0, it means the event X is impossible (i.e., I never wear those pants). On the other hand, if P(X)= 1 it means that event X is certain to occur (i.e., I always wear those pants).

**Law of total probability:** The probabilities of the elementary events need to add up to 1

#### Distributions

Let's take a look at this and see what is a distribution. 

```{r}
pants <- data.frame(
   type = c("Blue jeans","Grey jeans","Black jeans","Black suit","Blue tracksuit"),
   label = c("X1", "X2", "X3", "X4", "X5"),
   probability = c(0.5,0.3,0.1,0,0.1))

pants
```
Probability distribution is simply the probabilities of these different events above. Each of the events has a probability that lies between 0 and 1, and if we add up the probability of all events, they sum to 1.

```{r}
#Try plotting a bar graph of all the probabilities above
```


Now let's take a look at some distributions.

##### Generating Different types of distributions

**Normal Distribution**

One of the most common distributions in statistics is the ‘normal distribution’ aka Gaussian Distribution, well known as the ‘bell curve’ due to its characteristic bell shape.

Let's generate a Normal distribution: 

```{r}
# Here we are generating a normal distribution having 100 data points selected randomly. Try varying this number from 10 to 1000 and check the distribution plot.
x <- rnorm(100)
hist(x, col = 'steelblue')
abline(v = mean(x), lty = 2, lwd = 2)
```
How to generate a binomial distribution:
Here we have generated a 
```{r}
z<-rbinom( n = 100, size = 20, prob = 1/6 )

hist(z, col = 'steelblue')
```
Poisson Distribution
```{r}
y <- rpois(100, lambda = 1.2)
hist(y, col = 'steelblue')
abline(v = mean(y), lty = 2, lwd = 2)
```
Gamma Distribution
```{r}
x_dgamma <- seq(0, 2, by = 0.04)
y_dgamma <- dgamma(x_dgamma, shape = 6)
plot(y_dgamma)
```
mean as model
```{r}
z <- rnorm(50, mean = 5, sd = 2)
mean(z)
sd(z)
quantile(z)
quantile(z, 0.16)
mean(x) - sd(x)
mean(x) + sd(x)
```
Checking Normality of a distribution

```{r}
CompanyABCProfit<-read.csv("CompanyABCProfit.csv")
attach(CompanyABCProfit)
install.packages("dplyr")
install.packages("ggpubr")
library("ggpubr")
ggdensity(Profit,main="Profit per year", xlab="YEARLY PROFIT IN INR" )
ggqqplot(Profit)
ggplot(CompanyABCProfit) +     geom_point(mapping = aes(x = Year, y = Profit))
shapiro.test(Profit)
```

Generating a distribution and checking normality of the distribution
```{r}
N<- rnorm(100, mean=2, sd= 1.3 )
P <- rpois(100, lambda = 1.2)
#generate distribution z= p*N + (1-p)P
Y = 0.1*N
z= Y+P
#test Normality
shapiro.test(z) 
plot(z)
ggdensity(z)
ggqqplot(z)
```

```{r}
u=runif(40, min = -1, max = 1)
N<- rnorm(100, mean=2, sd= 1.3 )
#generate distribution z= p*N + (1-p)u where 0<p<1
s = 0.1*N
j=0.9*u
q= s+j
#test Normality
shapiro.test(q) 
plot(q)
ggdensity(q)
ggqqplot(q)
boxplot(q)
```
Emotional Valence Data
```{r}
# Load tidyverse and Warriner et al. (2013) data:

library(tidyverse)

war <- read_csv('warriner_2013_emotional_valence.csv')

# Check:

war

# Check valence measure range:

range(war$Val)

# Check the least and most positive wors:

filter(war, Val == min(Val) | Val == max(Val))

# Same thing, but more compact:

filter(war, Val %in% range(Val))

# Check tibble in ascending order:

arrange(war, Val)

# And descending order:

arrange(war, desc(Val))

#Mean and SD:

mean(war$Val)
sd(war$Val)

# 68% rule:

mean(war$Val) + sd(war$Val)
mean(war$Val) - sd(war$Val)

# Confirm:

quantile(war$Val, c(0.16, 0.84))

# Median:

median(war$Val)

# Which is the same as the 50th percentile:

quantile(war$Val, 0.5)
```

```{r}
ggdensity(war$Val)
boxplot(war$Val)
```
1. Generate an alpha distribution.
2. Generate a normal distribution with 150 data points and mean=2, standard deviation= 1.5 and a uniform distribution of maximum value 1, minimum value -1. Generate a distribution combining both using the equation, X= p*N + (p-1)u, where N= normal distribution, u= uniform distribution. X is generated new distribution. Vary the value of p= 1,2,3.. and check normality of distribution X. 
3. Load the emotional valence dataset from warrinar et al. 2013 check mean valence, plot boxplot with indication of first and third quantile value int it. Check normality of the data. 


